# 날짜: 2025-06-09

## 🕛 스크럼
- **오늘 할 일**: vLLM/Ollama 및 모델 최적화
- **예상 이유**: 현재 복잡한 요청에 대해 응답을 하지 않는 문제 있음. 때문에 가벼운 요청으로 도구 선정 및 최적화 진행할 것
- **어제 회고**: MCP 연결에 있어 npx가 필요하다는 걸 이전 테스트해서 확인했었는데 이를 제대로 기록하지 않아 시간을 지체함. 사소한 부분이라도 이를 꼼꼼히 기록해두고 이후에 비슷한 문제 발생시 바로 해결할 수 있도록 할 것.

- **팀 이슈**: 6/11 중간배포!
- **공지사항**: 13:25 PM 미팅 / 15:00 스프린트5 회고

<br>

## 💼 작업 내용
### 주제 1: vLLM/Ollama 테스트
- 평가항목: 응답시간, 초당 처리 토큰 수, 메모리 사용량, 응답 품질
- (작성중)

> 테스트 결과(단일요청, 간단한 프롬프트)

| 지표 | Ollama | vLLM(양자화) |
|------|--------|--------------|
| 응답 시간(sec) | 7.338<br>7.040<br>8.042<br>7.367<br>8.129 | 6.599<br>6.516<br>3.419<br>4.230<br>3.439 |
| 초당 처리 토큰 수 (token/sec) | input: 90<br>output: 187 / 25.484<br>output: 174 / 24.716<br>output: 216 / 26.860<br>output: 189 / 25.653<br>output: 215 / 26.448 | input: 90<br>output: 185 / 28.033<br>output: 185 / 28.390<br>output: 161 / 47.089<br>output: 200 / 47.287<br>output: 162 / 47.104 |
| 메모리 사용량 (MB) | 5353 | 3 → 5770 (모델 로딩) → 20898(KV 캐시 할당) |

> Ollama 출력

```
"title": "인공지능 분야 혁신, 새로운 산업 전환의 계기로 부상",\n  
"content": "최근 인공지능(AI) 기술의 발전은 여러 산업에 큰 영향을 미치고 있으며, 특히 기존 산업 구조를 재편하는 역할을 하고 있다. 한국 과학기술정보통신부는 AI 기술 개발과 활용 촉진을 위한 다양한 지원 정책을 추진 중이다. 산업통상자원부도 AI 기반의 제조현장 자동화 솔루션 개발에 박차를 가하고 있다. 이러한 추세에 따라 인공지능 기술은 앞으로 더욱 많은 산업 분야에서 핵심 역할을 수행하게 될 것으로 예상된다."
```

> vLLM 출력
```
# AWQ 양자화 모델
"title": "인공지능 기술, 혁신적 진보를 이끌다",\n  
"content": "최근 인공지능(AI) 기술이 급속도로 발전하며, 다양한 산업 분야에서 혁신을 이끌고 있습니다. 자연어 처리(NLP), 컴퓨터 비전, 머신러닝 등 AI의 핵심 기술들이 더욱 정교화되며, 이를 통해 디지털 트랜스포메이션의 속도가 빨라지고 있습니다. 특히, 의료, 자동차, 금융 등 주요 산업에서는 AI를 활용한 새로운 서비스와 제품이 잇따르고 있어, 기술의 변화가 삶의 질을 크게 향상시키는 데 기여하고 있습니다. "
```

> 테스트 결과(동시요청(5개), 간단한 프롬프트)

| 지표 | Ollama | vLLM(양자화) |
|------|--------|--------------|
| 응답 시간(sec) | 전체: 9.55<br>평균: 7.09<br>요청 1: 4.82<br>요청 2: 7.04<br>요청 3: 5.35<br>요청 4: 9.55<br>요청 5: 8.68 | 전체: 5.74<br>평균: 4.87<br>요청 1: 4.69초<br>요청 2: 4.52초<br>요청 3: 4.53초<br>요청 4: 4.86초<br>요청 5: 5.74초 |
| 평균 초당 처리 토큰 수 (token/sec) | 12.6 | 17.4 |
| 메모리 사용량 (MB) | 5353 | 3 → 5770 → 20898 |
| 응답 품질 | 요청5 중국어 응답 | 요청 1~4 중국어 응답 |

<br>

## ✊ 오늘의 도전 과제와 해결 방법
**도전과제 1**: 
* Langchain vLLM의 경우 클래스가 Chatmodel이 아닌 LLM이기 때문에 agent 호환이 안됨

**해결방법 1**: 
* vLLM을 OpenAI API 프로토콜을 모방하는 서버로 배포함으로써 chatmodel 클래스로 접근
* langchain-openai 패키지를 활용해 ChatOpenAI 클래스로 접근
```
from langchain_openai import ChatOpenAI

    # 모델 초기화
    inference_server_url = "http://localhost:8000/v1"
    vllm_llm = ChatOpenAI(
        model="Qwen/Qwen2.5-7B-Instruct",
        openai_api_key="EMPTY",
        openai_api_base=inference_server_url,
    )
```
* 별도의 터미널에서 vLLM 서버 실행
`vllm serve Qwen/Qwen2.5-7B-Instruct`


<br>

## 🤔 오늘의 회고(KPT)
- **Keep**: Ollama와 vLLM 중 어떤 서빙 툴을 사용할지 근거 마련
- **Problem**: Ollama가 기본적으로 양자화된 모델을 제공하고 이와 동일한 양자화된 모델이 vLLM(huggingface)에서 제공하지 않는 문제. 이에 따라 유사한 양자화 크기를 가진 모델로 비교함
- **Try**: 모델 최적화 관련해서 내일 추가로 진행 및 develop에 병합

<br>

## 🔗 내용정리 및 참고자료 링크
- [노션 정리](https://grizzly-crater-c04.notion.site/Ollama-vLLM-20d75a6ebc0a8055a5f7e4b7ca480a91?source=copy_link)
