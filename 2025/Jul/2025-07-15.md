# 날짜: 2025-07-15

## 🕛 스크럼
- **오늘 할 일**: 모델 서빙 최적화
- **예상 이슈**: AWQ 모델 양자화에 대한 러닝 커브
- **어제 회고**: vLLM 옵션 인자에 대한 테스트를 통해 서버 설정이나 기존 환경에서 크게 바꾸지 않는 선에서 가능한지 테스트. GPU 사용 제한, CPU를 같이 사용하는 등 테스트했지만 vLLM이 제공하는 서비스가 제한적이거나 병목 등의 이유로 사용 불가함을 확인
- **공지사항**: 13:15 PL 미팅 / (예정) 배포 전 QA

<br>

## 💼 작업 내용
### 주제 1: 모델 양자화
- autoawq 라이브러리를 통해 midm-2.0-Base-Instruct 모델에 대해 양자화 진행
- vLLM 서빙 과정에서 tool calling은 llama3_json 형식을 따라감
- 모델 로더, controller, service 모듈에 대해 코드 수정 및 swagger 테스트 후 정상 동작 확인
- 이전 transformers 사용시 run_in_threadpool로 동기-비동기 처리한 부분 삭제

<br>

## ✊ 오늘의 도전 과제와 해결 방법
**도전과제 1**: 
- 없음

**해결방법 1**: 
- 없음

<br>

## 🤔 오늘의 회고(KPT)
- **Keep**: -
- **Problem**: -
- **Try**: 양자화에 따른 모델 성능 저하 여부 확인을 위해 성능 테스트

<br>

