# 날짜: 2025-07-14

## 🕛 스크럼
- **오늘 할 일**: 모델 서빙 최적화
- **예상 이슈**: llama.cpp에서의 gpu 사용
- **어제 회고**: 지금까지의 생성 결과물에 대한 LLM 평가 및 문서정리
- **공지사항**: 11:00 모의면접 / 13:25 PM 미팅

<br>

## 💼 작업 내용
### 주제 1: 모델 서빙 최적화
- vLLM에서의 옵션 인자를 통해 사용 가능한지 테스트
- quantization: 동적 양자화가 아닌 양자화 모델을 불러오는 방식
- gpu_memory_utilization: 애초에 95% 이상으로 사용하여 VRAM 사용제한이 되지 않음
- cpu-offload-gb: KV캐시로 인해 상당량(10GB)을 CPU에 할당하니 병목으로 서버가 작동을 멈춤
- 이에 따라 모델을 직접 AWQ 양자화하여 서빙하는 것을 고려

<br>

## ✊ 오늘의 도전 과제와 해결 방법
**도전과제 1**: 
- 없음

**해결방법 1**: 
- 없음

<br>

## 🤔 오늘의 회고(KPT)
- **Keep**: -
- **Problem**: 모델 크기와 서버 VRAM 스펙에 따른 사용 제한 및 양자화 모델의 부재
- **Try**: 모델을 직접 학습 후 양자화 방식을 사용해 서빙 시도

<br>

## 🔗 내용정리 및 참고자료 링크
- [노션 정리](https://grizzly-crater-c04.notion.site/23075a6ebc0a80808863d06541b0abfb?source=copy_link)
